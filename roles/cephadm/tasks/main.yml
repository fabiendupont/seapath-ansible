# Copyright (C) 2025 RTE
# SPDX-License-Identifier: Apache-2.0

---
- include_vars: "{{ seapath_distro }}.yml"

# =============================================================================
# Registry Mode Detection
# =============================================================================
- name: Determine registry mode
  set_fact:
    is_control_registry: "{{ disconnected_mode | default(false) and registry_url != 'localhost:5000' }}"
    is_external_registry: "{{ not disconnected_mode | default(false) and registry_url != 'localhost:5000' }}"
    has_registry_mirror: "{{ registry_mirror_url is defined }}"
  run_once: true

# =============================================================================
# Prerequisites Validation
# =============================================================================
- name: Validate prerequisites
  block:
    - name: Check if required variables are set
      fail:
        msg: "Required variable '{{ item }}' is not set"
      when: 
        - cephadm_validate_prerequisites | default(true)
        - hostvars[inventory_hostname][item] is not defined or hostvars[inventory_hostname][item] == ""
      loop:
        - public_network
        - cluster_network
        - cluster_ip_addr
        - hostname

    - name: Check disk space (minimum 10GB)
      ansible.builtin.stat:
        path: /var/lib/ceph
      register: ceph_dir_stat
      when: cephadm_validate_prerequisites | default(true)

    - name: Fail if insufficient disk space
      fail:
        msg: "Insufficient disk space. Required: 10GB, Available: {{ (ceph_dir_stat.stat.size / 1024 / 1024 / 1024) | round }}GB"
      when: 
        - cephadm_validate_prerequisites | default(true)
        - (ceph_dir_stat.stat.size / 1024 / 1024 / 1024) | round < 10

    - name: Check memory (minimum 4GB)
      ansible.builtin.setup:
        filter: "ansible_memtotal_mb"
      when: cephadm_validate_prerequisites | default(true)

    - name: Fail if insufficient memory
      fail:
        msg: "Insufficient memory. Required: 4GB, Available: {{ (ansible_memtotal_mb / 1024) | round }}GB"
      when:
        - cephadm_validate_prerequisites | default(true)
        - (ansible_memtotal_mb / 1024) | round < 4

    - name: Check container runtime
      ansible.builtin.which:
        program: podman
      register: podman_check
      when: cephadm_validate_prerequisites | default(true)

    - name: Fail if podman not found
      fail:
        msg: "Podman container runtime not found. Please install podman."
      when:
        - cephadm_validate_prerequisites | default(true)
        - podman_check is failed

# =============================================================================
# User and Group Management
# =============================================================================
- name: Create Ceph Users and Groups
  block:
    - name: Ensure group "cephadm" exists
      group:
        name: cephadm
        gid: "{{ cephadm_group_id }}"
        state: present

    - name: Ensure user "cephadm" exists
      user:
        name: cephadm
        uid: "{{ cephadm_user_id }}"
        group: cephadm
        create_home: yes

    - name: Set cephadm user sudo permissions
      copy:
        src: cephadm_sudoers
        dest: /etc/sudoers.d/cephadm

- name: Create Debian-specific Ceph Users and Groups
  block:
    - name: Ensure group "containerized-ceph" exists
      group:
        name: containerized-ceph
        gid: "{{ containerized_ceph_group_id }}"
        state: present

    - name: Ensure user "containerized-ceph" exists with nologin
      user:
        name: containerized-ceph
        uid: "{{ containerized_ceph_user_id }}"
        group: containerized-ceph
        create_home: no
        shell: /sbin/nologin
  when: seapath_distro == "Debian"

# =============================================================================
# Cephadm Installation
# =============================================================================
- name: Install Cephadm Binary and Repository
  block:
    - name: Download cephadm
      get_url:
        url: "https://download.ceph.com/rpm-{{ cephadm_release }}/el9/noarch/cephadm"
        dest: "/tmp/cephadm"
        mode: '0755'

    - name: Install cephadm repository
      command: "{{ '/tmp/cephadm' if cephadm_install | default(false) else 'cephadm' }} add-repo --release squid"
      changed_when: true

    - name: Install cephadm
      command: /tmp/cephadm install
      changed_when: true
  when: cephadm_install is true

- name: Install Ceph Common Package
  command: cephadm install ceph-common
  changed_when: true
  when: cephadm_installcommon is true

# =============================================================================
# Registry Authentication
# =============================================================================
- name: Login to external registry
  command: >
    cephadm registry-login
    --registry-url {{ registry_url }}
    --username {{ registry_username }}
    --password {{ registry_password }}
  when: 
    - (is_control_registry or is_external_registry)
    - registry_username != ""
    - registry_password != ""
  no_log: true

# =============================================================================
# Cluster State Detection and Management
# =============================================================================
- name: Detect Cluster State and Organize Nodes
  block:
    - name: Check if host is part of a Ceph cluster
      ceph.automation.ceph_status:
        cluster: ceph
      register: ceph_status
      ignore_errors: true
      changed_when: false

    - name: Mark host as ceph or nonceph
      set_fact:
        is_ceph_node: "{{ ceph_status is not failed }}"

    - name: Add host to ceph_nodes group
      add_host:
        name: "{{ item }}"
        groups: ceph_nodes
      loop: "{{ groups['cluster_machines'] }}"
      when: hostvars[item].is_ceph_node

    - name: Add host to nonceph_nodes group
      add_host:
        name: "{{ item }}"
        groups: nonceph_nodes
      loop: "{{ groups['cluster_machines'] }}"
      when: not hostvars[item].is_ceph_node

    - name: Set first_node and bootstrap condition
      set_fact:
        first_node: >-
          {{ (groups['ceph_nodes'][0] if (groups['ceph_nodes'] is defined and groups['ceph_nodes'] | length > 0)
             else groups['cluster_machines'][0]) | trim }}
        do_bootstrap: "{{ (groups['ceph_nodes'] | default([])) | length == 0 }}"

    - name: Compute list of nodes to add as monitors
      set_fact:
        mon_nodes_to_add: "{{ (groups['nonceph_nodes'] | default([]) | difference([first_node])) if do_bootstrap else groups['nonceph_nodes'] | default([]) }}"

    - name: Debug cluster state
      debug:
        msg: |
          first_node = {{ first_node }}
          do_bootstrap = {{ do_bootstrap }}
          mon_nodes_to_add = {{ mon_nodes_to_add | default([]) }}
          ceph_nodes = {{ groups['ceph_nodes'] | default([]) }}
          nonceph_nodes = {{ groups['nonceph_nodes'] | default([]) }}
          registry_url = {{ registry_url }}
          registry_host = {{ registry_host }}
          registry_mirror_url = {{ registry_mirror_url | default('not set') }}
          is_control_registry = {{ is_control_registry }}
          is_external_registry = {{ is_external_registry }}
          has_registry_mirror = {{ has_registry_mirror }}
  run_once: true

# =============================================================================
# Ceph Cluster Bootstrap
# =============================================================================
- name: Bootstrap Ceph cluster
  block:
    - name: Create ceph.conf template
      template:
        src: ceph.conf.j2
        dest: /tmp/ceph.conf
        mode: 0644
      run_once: true
      delegate_to: "{{ first_node }}"

    - name: Bootstrap Ceph cluster using native module
      ceph.automation.cephadm_bootstrap:
        mon_ip: "{{ hostvars[first_node]['cluster_ip_addr'] }}"
        image: "quay.io/ceph/ceph:v{{ cephadm_release }}"
        config: /tmp/ceph.conf
        ssh_user: cephadm
        skip_monitoring_stack: true
        skip_dashboard: true
        skip_firewalld: true
      register: ceph_bootstrap_result
      retries: 3
      delay: 10
      until: ceph_bootstrap_result is not failed
      run_once: true
      delegate_to: "{{ first_node }}"

    - name: Display bootstrap result
      debug:
        msg: "Ceph cluster bootstrap completed successfully"
      when: ceph_bootstrap_result is not failed
      run_once: true

    - name: Fail on bootstrap error
      fail:
        msg: "Ceph cluster bootstrap failed: {{ ceph_bootstrap_result.msg | default('Unknown error') }}"
      when: ceph_bootstrap_result is failed
      run_once: true
  when: do_bootstrap | bool

# =============================================================================
# SSH Key Management for Additional Nodes
# =============================================================================
- name: Manage SSH keys for additional nodes
  block:
    - name: Check if /etc/ceph/ceph.pub exists on first_node
      stat:
        path: /etc/ceph/ceph.pub
      register: ceph_pub_stat
      run_once: true
      delegate_to: "{{ first_node }}"

    - name: Fetch the ceph keyfile if it exists, otherwise fetch authorized_keys
      fetch:
        src: "{{ ceph_pub_stat.stat.exists | ternary('/etc/ceph/ceph.pub', '/home/cephadm/.ssh/authorized_keys') }}"
        dest: "/tmp/ceph.pub"
        flat: true
      delegate_to: "{{ first_node }}"
      run_once: true

    - name: Read the key from the local file
      set_fact:
        ceph_pubkey: "{{ lookup('file', '/tmp/ceph.pub') }}"
      delegate_to: localhost
      run_once: true

    - name: Add the ceph pubkey to each target node
      ansible.posix.authorized_key:
        user: cephadm
        state: present
        key: "{{ ceph_pubkey }}"
      with_items: "{{ mon_nodes_to_add }}"
      loop_control:
        loop_var: target_node
      delegate_to: "{{ target_node }}"
      run_once: true
  when: 
    - mon_nodes_to_add | length > 0
    - cephadm_ssh_key_management | default(true)

# =============================================================================
# Monitor Management
# =============================================================================
- name: Add hosts to Ceph orchestrator with _admin label
  ceph.automation.ceph_orch_host:
    name: "{{ hostvars[item]['hostname'] }}"
    labels: ["_admin"]
    state: present
  loop: "{{ mon_nodes_to_add }}"
  run_once: true
  delegate_to: "{{ first_node }}"
  when: mon_nodes_to_add | length > 0

- name: Wait for monitors to be added to monmap
  ceph.automation.ceph_mon:
    state: present
  register: mon_dump
  retries: 30
  delay: 5
  changed_when: false
  run_once: true
  delegate_to: "{{ first_node }}"
  until: mon_dump.mons is defined and hostvars[item]['hostname'] in mon_dump.mons
  loop: "{{ mon_nodes_to_add }}"
  when: mon_nodes_to_add | length > 0

# =============================================================================
# OSD Management
# =============================================================================
- name: Deploy OSD Daemons
  block:
    - name: Get list of current OSD daemons and their hosts
      ceph.automation.ceph_orch_ps:
        daemon_type: osd
      register: osd_ps
      delegate_to: "{{ first_node }}"
      changed_when: false

    - name: Set fact for existing OSD hosts
      set_fact:
        existing_osd_hosts: "{{ osd_ps.daemons | map(attribute='hostname') | list | unique }}"

    - name: Set list of nodes that need OSDs
      set_fact:
        nodes_needing_osds: "{{ groups['cluster_machines'] | map('extract', hostvars, 'hostname') | difference(existing_osd_hosts) }}"

    - name: Copy ceph orch spec file
      template:
        src: "{{ ceph_spec_path | default('spec.yaml.j2') }}"
        dest: /tmp/t.yaml
        mode: 0644
      delegate_to: "{{ first_node }}"

    - name: Add OSD daemon on nodes that need OSDs
      ceph.automation.ceph_orch_apply:
        spec: /tmp/t.yaml
      delegate_to: "{{ first_node }}"
      failed_when: false
  run_once: true

# =============================================================================
# LVM Volume Expansion Detection and Management
# =============================================================================
- name: Detect and Prepare LVM Expansion
  block:
    - name: Refresh LVM facts for expansion detection
      ansible.builtin.setup:
        filter: "ansible_lvm"

    - name: Detect if VG expansion is needed
      set_fact:
        vg_expansion_needed: "{{ lvm_volumes is defined and ansible_lvm is defined }}"

    - name: Detect if LV expansion is needed
      set_fact:
        lv_expansion_needed: >-
          {{ lvm_volumes is defined and ansible_lvm is defined and 
             (lvm_volumes[0].data_size | int) > 
             ((ansible_lvm['lvs'][lvm_volumes[0].data]['size_g'] | float * 1024) | round(2)) }}

    - name: Debug expansion requirements
      debug:
        msg: |
          VG expansion needed: {{ vg_expansion_needed | default(false) }}
          LV expansion needed: {{ lv_expansion_needed | default(false) }}
          Current LV size: {{ (ansible_lvm['lvs'][lvm_volumes[0].data]['size_g'] | float * 1024) | round(2) }}MB
          Required LV size: {{ lvm_volumes[0].data_size }}MB
      when: ansible_verbosity >= 2
  when: lvm_volumes is defined

# =============================================================================
# VG Expansion (Physical Volume and Volume Group)
# =============================================================================
- name: Expand Volume Group
  block:
    - name: Get PV information for VG expansion
      set_fact:
        pv_list: "{{ ansible_lvm['pvs'] | dict2items | community.general.json_query(query) }}"
      vars:
        query: "[?value.vg=='{{ lvm_volumes[0].data_vg }}'].key"

    - name: Get partition information for PV expansion
      community.general.parted:
        device: "{{ item | regex_replace('(p?\\d+)$', '') }}"
      loop: "{{ pv_list }}"
      register: parted_info

    - name: Calculate new partition sizes
      set_fact:
        partition_expansions: "{{ parted_info.results | map('extract', 'ansible_facts') | list }}"

    - name: Expand partitions declaratively
      community.general.parted:
        device: "{{ item.device }}"
        number: "{{ item.partition_num }}"
        part_end: "{{ item.new_end }}MiB"
        state: present
      loop: "{{ partition_expansions }}"

    - name: Resize physical volumes
      community.general.pvresize:
        pv: "{{ item }}"
      loop: "{{ pv_list }}"
  when: vg_expansion_needed | default(false)

# =============================================================================
# LV Expansion and OSD Resizing
# =============================================================================
- name: Expand Logical Volume and Resize OSDs
  block:
    - name: Expand logical volume declaratively
      community.general.lvol:
        vg: "{{ lvm_volumes[0].data_vg }}"
        lv: "{{ lvm_volumes[0].data }}"
        size: "{{ lvm_volumes[0].data_size }}"
      register: lv_expansion_result

    - name: Handle OSD resizing after LV expansion
      block:
        - name: Get OSD service information for resizing
          ansible.builtin.service_facts:

        - name: Find Ceph OSD services
          set_fact:
            osd_services: "{{ ansible_facts.services | dict2items | selectattr('key', 'match', '.*osd.*') | map(attribute='key') | list }}"

        - name: Stop OSD services for resizing
          ansible.builtin.systemd:
            name: "{{ item }}"
            state: stopped
          loop: "{{ osd_services }}"

        - name: Wait for OSD services to stop
          ansible.builtin.wait_for:
            timeout: 5

        - name: Resize OSDs using cephadm shell
          ansible.builtin.command: >
            cephadm shell --name {{ item | regex_search('@(osd\\.[0-9]+)\\.service', '\\1') | list | first }}
            ceph-bluestore-tool bluefs-bdev-expand
            --path /var/lib/ceph/osd/ceph-{{ (item | regex_search('@(osd\\.[0-9]+)\\.service', '\\1') | list | first).split('.')[-1] }}
          loop: "{{ osd_services }}"

        - name: Start OSD services after resizing
          ansible.builtin.systemd:
            name: "{{ item }}"
            state: started
          loop: "{{ osd_services }}"

        - name: Wait for cluster health after OSD resizing
          ceph.automation.ceph_status:
            cluster: ceph
          register: cluster_health_after_resize
          retries: 20
          delay: 2
          changed_when: false
          until: cluster_health_after_resize.health.status == "HEALTH_OK"
      when: lv_expansion_result.changed | default(false)
  when: lv_expansion_needed | default(false)

# =============================================================================
# Cluster Health Validation and Final Configuration
# =============================================================================
- name: Validate Cluster Health and Configure Resources
  block:
    - name: Wait for cluster to be healthy
      ceph.automation.ceph_status:
        cluster: ceph
      register: cluster_health
      retries: "{{ cephadm_health_check_retries }}"
      delay: "{{ cephadm_health_check_delay }}"
      changed_when: false
      delegate_to: "{{ first_node }}"
      until: cluster_health.health.status == "HEALTH_OK"

    - name: Display cluster health status
      debug:
        msg: "Cluster health: {{ cluster_health.health.status }}"

    - name: Create RBD pool if it doesn't exist
      ceph.automation.ceph_pool:
        name: rbd
        state: present
        application: rbd
      delegate_to: "{{ first_node }}"

    - name: Create or update CephX user client.libvirt
      ceph.automation.ceph_key:
        name: client.libvirt
        caps:
          mon: 'profile rbd, allow command "osd blacklist"'
          osd: 'allow class-read object_prefix rbd_children, profile rbd pool=rbd'
        state: present
  run_once: true